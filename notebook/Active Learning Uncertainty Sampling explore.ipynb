{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning Overview\n",
    "## Step 0: Gather Data\n",
    "## Step 1: Split into Seed and Unlabelled Datasets\n",
    "## Step 2: Train the Model\n",
    "## Step 3: Choose unlabelled instances\n",
    "### 1. Determine the type of scenario:\n",
    "* [ ] Membership Query Synthesis\n",
    "* [ ] Stream-Based Selective Sampling\n",
    "* [X] Pool-Based sampling \n",
    "\n",
    "### 2. query strategy\n",
    "* [X] Uncertainty Sampling\n",
    "* [ ] Query-By-Committee\n",
    "* [ ] Expected Model Change\n",
    "* [ ] Expected Error Reduction\n",
    "* [ ] Variance Reduction\n",
    "* [ ] Density-Weighted Methods\n",
    "\n",
    "## Step 4: Stopping criteria\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.526978Z",
     "start_time": "2021-04-02T13:28:08.524808Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC, LinearSVC\n",
    "# from sklearn import svm\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from collections import Counter\n",
    "# import matplotlib.pyplot as plt\n",
    "# from plotly.subplots import make_subplots\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from typing import Callable,List,Any\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import random\n",
    "# from sklearn.metrics import f1_score\n",
    "# from functools import wraps\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import nltk\n",
    "# import copy\n",
    "# import seaborn as sns\n",
    "# from scipy.interpolate import griddata\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Gather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.542839Z",
     "start_time": "2021-04-02T13:28:08.539139Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Read data into a DataFrame\n",
    "# origdata = pd.read_csv('/etlstage/PEE_joint/NUS_modules/CS5339/project/Iris.csv')\n",
    "# display(origdata.head())\n",
    "\n",
    "# # Read data into a DataFrame\n",
    "# titanic = pd.read_csv('/etlstage/PEE_joint/NUS_modules/CS5340/group_data/small.csv')\n",
    "# display(titanic.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.548361Z",
     "start_time": "2021-04-02T13:28:08.544058Z"
    }
   },
   "outputs": [],
   "source": [
    "# def Initialization(clf,QueryStrategy):\n",
    "#     k1, k2 = 'PetalLengthCm', 'PetalWidthCm'\n",
    "#     data = origdata[[k1, k2, 'Species']].copy()\n",
    "\n",
    "#     X = data[[k1, k2]]\n",
    "#     y = data['Species']\n",
    "#     print('Classes:')\n",
    "#     # print(y.unique(), '\\n\\n\\n')\n",
    "\n",
    "#     y[y=='Iris-setosa'] = 0\n",
    "#     y[y=='Iris-versicolor'] = 1\n",
    "#     y[y=='Iris-virginica'] = 2\n",
    "\n",
    "#     plt.figure()\n",
    "#     setosa = y == 0\n",
    "#     versicolor = y == 1\n",
    "#     virginica = y == 2\n",
    "\n",
    "#     X1 = X[y != 0]\n",
    "#     y1 = y[y != 0]\n",
    "\n",
    "#     X1 = X1.reset_index(drop=True)\n",
    "#     y1 = y1.reset_index(drop=True)\n",
    "#     y1 -= 1\n",
    "#     y1 = y1.astype(dtype=np.uint8)\n",
    "    \n",
    "#     X_pool, X_test, y_pool, y_test = train_test_split(X1, y1, test_size=0.2, random_state=2)\n",
    "#     X_pool, X_test, y_pool, y_test = X_pool.reset_index(drop=True), X_test.reset_index(drop=True), y_pool.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "#     pool_datset = X_pool.copy()\n",
    "#     pool_datset['y'] = y_pool\n",
    "\n",
    "#     if \"QBC\" in QueryStrategy:\n",
    "#         clf0 = clf[0]\n",
    "#     else:\n",
    "#         clf0 = clf\n",
    "        \n",
    "#     clf0.fit(X1, y1)\n",
    "    \n",
    "#     if \"QBC\" in QueryStrategy:\n",
    "#         model = clf[0]\n",
    "#     else:\n",
    "#         model = clf\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     print(f\"f1_score = {f1_score(y_test, y_pred, average='macro')}\")\n",
    "\n",
    "#     print(clf0.coef_)\n",
    "#     print(clf0.intercept_)\n",
    "\n",
    "#     xmin, xmax = X1[k1].min(), X1[k1].max()\n",
    "#     ymin, ymax = X1[k2].min(), X1[k2].max()\n",
    "    \n",
    "#     stepx = (xmax - xmin)/99\n",
    "#     stepy = (ymax - ymin)/99\n",
    "\n",
    "\n",
    "#     a0, b0, c0 = clf0.coef_[0, 0], clf0.coef_[0, 1], clf0.intercept_\n",
    "\n",
    "#     # a*x + b*y + c = 0\n",
    "#     # y = -(a*x + c)/b\n",
    "\n",
    "#     lx0 = [xmin + stepx * i for i in range(100)]\n",
    "#     ly0 = [-(a0*lx0[i] + c0)/b0 for i in range(100)]\n",
    "    \n",
    "#     ly0min, ly0max = min(ly0), max(ly0)\n",
    "#     if (ymax - ymin) < (ly0max - ly0min):\n",
    "#         indx_rng = [ly0.index(i) for i in ly0 if (i<=ymax)&(i>=ymin)]\n",
    "#         print(indx_rng)\n",
    "#         lx0 = [lx0[i] for i in indx_rng]\n",
    "#         ly0 = [ly0[i] for i in indx_rng]\n",
    "#     plt.figure()\n",
    "\n",
    "#     # k1, k2 = k2, k1\n",
    "#     plt.title(\"using full dataset and SVC to plot true decision boundary\")\n",
    "#     # plt.scatter(x[k1][setosa], x[k2][setosa], c='r')\n",
    "#     plt.scatter(X1[k1][y1==0], X1[k2][y1==0], c='r')\n",
    "#     plt.scatter(X1[k1][y1==1], X1[k2][y1==1], c='c')\n",
    "\n",
    "#     plt.plot(lx0, ly0, c='m')\n",
    "\n",
    "#     plt.xlabel(k1)\n",
    "#     plt.ylabel(k2)\n",
    "\n",
    "#     plt.show()\n",
    "    \n",
    "#     return  k1, k2, X1, y1,lx0,ly0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.555292Z",
     "start_time": "2021-04-02T13:28:08.549534Z"
    }
   },
   "outputs": [],
   "source": [
    "# def Initialization(clf,QueryStrategy):\n",
    "#     k1, k2 = 'feature1', 'feature2'\n",
    "#     data = dataset[[k1, k2, 'class']].copy()\n",
    "\n",
    "#     X = data[[k1, k2]]\n",
    "#     y = data['class']\n",
    "#     print('Classes:')\n",
    "#     print(y.unique(), '\\n\\n\\n')\n",
    "\n",
    "#     plt.figure()\n",
    "\n",
    "#     X1 = X\n",
    "#     y1 = y\n",
    "\n",
    "#     X_pool, X_test, y_pool, y_test = train_test_split(X1, y1, test_size=0.2, random_state=100)\n",
    "#     X_pool, X_test, y_pool, y_test = X_pool.reset_index(drop=True), X_test.reset_index(drop=True), y_pool.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "#     pool_datset = X_pool.copy()\n",
    "#     pool_datset['class'] = y_pool\n",
    "\n",
    "#     if \"QBC\" in QueryStrategy:\n",
    "#         clf0 = clf[0]\n",
    "#     else:\n",
    "#         clf0 = clf\n",
    "\n",
    "#     clf0.fit(X1, y1)\n",
    "\n",
    "#     if \"QBC\" in QueryStrategy:\n",
    "#         model = clf[0]\n",
    "#     else:\n",
    "#         model = clf\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     print(f\"f1_score = {f1_score(y_test, y_pred, average='macro')}\")\n",
    "\n",
    "#     print(clf0.coef_)\n",
    "#     print(clf0.intercept_)\n",
    "\n",
    "#     xmin, xmax = X1[k1].min(), X1[k1].max()\n",
    "#     ymin, ymax = X1[k2].min(), X1[k2].max()\n",
    "\n",
    "#     stepx = (xmax - xmin)/99\n",
    "#     stepy = (ymax - ymin)/99\n",
    "\n",
    "\n",
    "#     a0, b0, c0 = clf0.coef_[0, 0], clf0.coef_[0, 1], clf0.intercept_\n",
    "\n",
    "#     # a*x + b*y + c = 0\n",
    "#     # y = -(a*x + c)/b\n",
    "\n",
    "#     lx0 = [xmin + stepx * i for i in range(100)]\n",
    "#     ly0 = [-(a0*lx0[i] + c0)/b0 for i in range(100)]\n",
    "\n",
    "#     ly0min, ly0max = min(ly0), max(ly0)\n",
    "#     if (ymax - ymin) < (ly0max - ly0min):\n",
    "#         indx_rng = [ly0.index(i) for i in ly0 if (i<=ymax)&(i>=ymin)]\n",
    "#         print(indx_rng)\n",
    "#         lx0 = [lx0[i] for i in indx_rng]\n",
    "#         ly0 = [ly0[i] for i in indx_rng]\n",
    "#     plt.figure()\n",
    "\n",
    "#     # k1, k2 = k2, k1\n",
    "#     plt.title(\"using full dataset and SVC to plot true decision boundary\")\n",
    "#     # plt.scatter(x[k1][setosa], x[k2][setosa], c='r')\n",
    "#     plt.scatter(X1[k1][y1==0], X1[k2][y1==0], c='r')\n",
    "#     plt.scatter(X1[k1][y1==1], X1[k2][y1==1], c='c')\n",
    "\n",
    "#     plt.plot(lx0, ly0, c='m')\n",
    "\n",
    "#     plt.xlabel(k1)\n",
    "#     plt.ylabel(k2)\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "#     return  k1, k2, X1, y1,lx0,ly0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.560902Z",
     "start_time": "2021-04-02T13:28:08.556370Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_classification\n",
    "\n",
    "# X, y = make_classification(n_samples=1000,n_features=2, n_redundant=0, n_informative=2,\n",
    "#                            random_state=324231432, n_clusters_per_class=2,class_sep=1.5)#3423\n",
    "\n",
    "# # X, y = make_classification(n_samples=1000,n_features=2, n_redundant=0, n_informative=2,\n",
    "# #                            random_state=4124235443, n_clusters_per_class=1,class_sep=2)#3423\n",
    "\n",
    "# # X1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=1,\n",
    "# #                              n_clusters_per_class=1)\n",
    "\n",
    "# dataset = pd.DataFrame(data=X, columns=['feature1','feature2'])\n",
    "# dataset['class'] = y\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.566836Z",
     "start_time": "2021-04-02T13:28:08.562259Z"
    }
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# clf = SVC(kernel='linear',probability=True)\n",
    "\n",
    "# k1, k2, X1, y1,lx0,ly0 = Initialization(clf,'Least_Confident')\n",
    "\n",
    "# X_pool, X_test, y_pool, y_test = train_test_split(X1, y1, test_size=0.2, random_state=2)\n",
    "# X_pool, X_test, y_pool, y_test = X_pool.reset_index(drop=True), X_test.reset_index(drop=True), y_pool.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "# pool_datset = X_pool.copy()\n",
    "# pool_datset['y'] = y_pool\n",
    "\n",
    "# print(f\"X_pool,y_pool: trainning data pool for unlabelled/labelled data ; size = {len(X_pool)}\")\n",
    "# print(f\"X_test,y_test: test data; size = {len(X_test)}\")\n",
    "# display(pool_datset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.572505Z",
     "start_time": "2021-04-02T13:28:08.568082Z"
    }
   },
   "outputs": [],
   "source": [
    "# class active_learning_base():\n",
    "#     def __init__(self, QueryStrategy):\n",
    "#         self.QueryStrategy = QueryStrategy\n",
    "#         self.Query_Strategy_Frameworks={}\n",
    "        \n",
    "#     def add_Query_Strategy(self, cls):\n",
    "#         def decorator(func):\n",
    "#             self.Query_Strategy_Frameworks[func.__name__] = func\n",
    "#             @wraps(func)\n",
    "#             def wrapper(self, *args, **kwargs): \n",
    "#                 return func(*args, **kwargs)\n",
    "#             setattr(cls, func.__name__, wrapper)\n",
    "#             return func\n",
    "#         return decorator\n",
    "    \n",
    "#     def error_logging(self,clf,X_test,y_test):\n",
    "#         if \"QBC\" in self.QueryStrategy:\n",
    "#             model = clf[0]\n",
    "#         else:\n",
    "#             model = clf\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         matrix = confusion_matrix(y_test, y_pred)\n",
    "#         #print(matrix)\n",
    "#         #print(matrix[0][1]+matrix[1][0])\n",
    "#         return f1_score(y_test, y_pred, average='macro'), matrix[0][1]+matrix[1][0]\n",
    "        \n",
    "#     def plot_svm(self,clf,X_pool,y_pool, train_indexes, unknown_indexes, log_preference, new_index = False, title = False, name = False):\n",
    "#         X_train = X_pool.iloc[train_indexes]\n",
    "#         y_train = y_pool.iloc[train_indexes]\n",
    "\n",
    "#         X_unk = X_pool.iloc[unknown_indexes]\n",
    "#         # y_unk = y_pool.iloc[unknown_indexes]\n",
    "#         log_preference.loc[X_pool.index.isin(train_indexes),'preference'] += 1\n",
    "#         #display(log_preference)\n",
    "        \n",
    "#         if new_index:\n",
    "#             X_new = X_pool.iloc[new_index]\n",
    "            \n",
    "#         if \"QBC\" in self.QueryStrategy:\n",
    "#             model = clf[0]\n",
    "#         else:\n",
    "#             model = clf\n",
    "\n",
    "#         a, b, c = model.coef_[0, 0], model.coef_[0, 1], model.intercept_\n",
    "        \n",
    "#        # print(a, b, c)\n",
    "#         # a*x + b*y + c = 0\n",
    "#         # y = -(a*x + c)/b\n",
    "#         xmin, xmax = X_pool[k1].min(), X_pool[k1].max()\n",
    "#         ymin, ymax = X_pool[k2].min(), X_pool[k2].max()\n",
    "#         stepx = (xmax - xmin)/99\n",
    "#         stepy = (ymax - ymin)/99\n",
    "        \n",
    "#         lx = [xmin + stepx * i for i in range(100)]\n",
    "#         ly = [-(a*lx[i] + c)/b for i in range(100)]\n",
    "\n",
    "#         lymin, lymax = min(ly), max(ly)\n",
    "#         if (ymax - ymin) < (lymax - lymin):\n",
    "#             indx_rng = [ly.index(i) for i in ly if (i<=ymax)&(i>=ymin)]\n",
    "#             #print(indx_rng)\n",
    "#             lx = [lx[i] for i in indx_rng]\n",
    "#             ly = [ly[i] for i in indx_rng]\n",
    "#         plt.figure()\n",
    "        \n",
    "#         #fig = plt.figure(figsize=(9,6))\n",
    "#         fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18,6))\n",
    "        \n",
    "#         # Construct 2D histogram from data using the 'plasma' colormap\n",
    "#         x=log_preference[k1]\n",
    "#         y=log_preference[k2]\n",
    "#         z=log_preference[\"preference\"]\n",
    "#         xi=np.linspace(-4.,4.,100)\n",
    "#         yi=np.linspace(-4.,4.,100)\n",
    "#         X,Y= np.meshgrid(xi,yi)\n",
    "#         Z = griddata((x, y), z, (X, Y),method='linear')\n",
    "#         Z[np.isnan(Z)] = 0\n",
    "#         im = axes[0].pcolormesh(X,Y,Z, cmap = plt.get_cmap('plasma'))\n",
    "#         fig.colorbar(im, ax=axes[0],label='Query Priority' )\n",
    "#         #heat = log_preference.pivot(k1, k2, \"preference\")\n",
    "#         #sns.heatmap(heat,ax=axes[0], xticklabels=100, yticklabels=100)\n",
    "# #         if len(X_train)<=50:\n",
    "# #             N_bins = len(X_train)\n",
    "# #         elif (len(X_train)>50)&(len(X_train)<=100):\n",
    "# #             N_bins = int(len(X_train)/2)\n",
    "# #         elif (len(X_train)>100)&(len(X_train)<=300):\n",
    "# #             N_bins = 30\n",
    "# #         elif (len(X_train)>300)&(len(X_train)<=600):\n",
    "# #             N_bins = 40\n",
    "# #         elif (len(X_train)>600)&(len(X_train)<=1000):\n",
    "# #             N_bins = 60\n",
    "        \n",
    "# #         axes[0].hist2d(X_train[k1], X_train[k2], bins=N_bins, normed=False, cmap='plasma')\n",
    "\n",
    "#         # plt.scatter(x[k1][setosa], x[k2][setosa], c='r')\n",
    "#         axes[1].scatter(X_unk[k1], X_unk[k2], c='k', marker = '.', label=\"unlablled\")\n",
    "#         axes[1].scatter(X_train[k1][y_train==0], X_train[k2][y_train==0], c='r', marker = 'o', label=\"lablled\")\n",
    "#         axes[1].scatter(X_train[k1][y_train==1], X_train[k2][y_train==1], c='c', marker = 'o', label=\"lablled\")\n",
    "\n",
    "#         axes[1].plot(lx0, ly0, '--', c='g', label=\"True DB\")    \n",
    "#         axes[1].plot(lx, ly, c='m', label=\"new DB\")\n",
    "\n",
    "#         if new_index:\n",
    "#             axes[1].scatter(X_new[k1], X_new[k2], c='y', marker=\"*\", s=125, label=\"new lablled\")\n",
    "#             axes[1].scatter(X_new[k1], X_new[k2], c='y', marker=\"*\", s=125, label=\"new lablled\")\n",
    "#             axes[1].scatter(X_new[k1], X_new[k2], c='y', marker=\"*\", s=125, label=\"new lablled\")\n",
    "#             axes[1].scatter(X_new[k1], X_new[k2], c='y', marker=\"*\", s=125, label=\"new lablled\")\n",
    "#             axes[1].scatter(X_new[k1], X_new[k2], c='y', marker=\"*\", s=125, label=\"new lablled\")\n",
    "\n",
    "#         if title:\n",
    "#             plt.title(title)\n",
    "\n",
    "#         plt.xlabel(k1)\n",
    "#         plt.ylabel(k2)\n",
    "#         handles, labels = plt.gca().get_legend_handles_labels()\n",
    "#         by_label = dict(zip(labels, handles))\n",
    "#         plt.legend(by_label.values(), by_label.keys())\n",
    "        \n",
    "#         if title in [\"Iteration 3\",\"Iteration 6\",\"Iteration 20\",\"Iteration 200\",\"Iteration 550\"]:\n",
    "#             plt.savefig(f'/etlstage/PEE_joint/NUS_modules/CS5339/project/image/{self.QueryStrategy}_{title}.png')\n",
    "#         plt.show()\n",
    "#         return X_pool\n",
    "    \n",
    "#     def active_learner(self,clf,train_size,\n",
    "#                        X_pool,y_pool,X_test,y_test,\n",
    "#                        budget,if_plot=False):#def active_learner(self, **fit_kwargs,)\n",
    "#         score = []\n",
    "#         no_of_err = []\n",
    "#         train_indexes=[]\n",
    "#         init_cls_size = int(train_size/len(set(y_pool)))\n",
    "#         # generate a cold start\n",
    "        \n",
    "#         for cls in set(y_pool):\n",
    "#             X_pool_cls = X_pool.iloc[y_pool[(y_pool==cls)].index]\n",
    "#             train_indexes.append(X_pool_cls[(X_pool_cls[k1] == X_pool_cls[k1].max())].index[0])\n",
    "# #         for cls in set(y_pool):\n",
    "# #             for i in range(init_cls_size):\n",
    "# #                 train_indexes.append(y_pool[(y_pool==cls)].index[i])\n",
    "                \n",
    "#         unknown_indexes = [unlabeled_idx for unlabeled_idx in list(range(len(X_pool))) if unlabeled_idx not in train_indexes]\n",
    "#         X_train = X_pool.iloc[train_indexes]\n",
    "#         y_train = y_pool.iloc[train_indexes]\n",
    "        \n",
    "#         if \"QBC\" in self.QueryStrategy:\n",
    "#             for model in clf:\n",
    "#                 model.fit(X_train, y_train)\n",
    "#         else:\n",
    "#             clf.fit(X_train, y_train)\n",
    "        \n",
    "#         sample_preference = X_pool.copy()\n",
    "#         sample_preference['preference'] = np.zeros(len(X_pool))\n",
    "        \n",
    "#         if if_plot:\n",
    "#             title = \"Cold start\"\n",
    "#             self.plot_svm(clf, X_pool,y_pool, train_indexes, unknown_indexes,sample_preference, False, title)\n",
    "            \n",
    "#         f1_score,mis_cls = self.error_logging(clf,X_pool,y_pool)\n",
    "#         #f1_score,mis_cls = self.error_logging(clf,X_test,y_test,)\n",
    "#         score.append(f1_score)\n",
    "#         no_of_err.append(mis_cls)\n",
    "        \n",
    "#         n = self.Query_Strategy_Frameworks[self.QueryStrategy](clf,X_pool,unknown_indexes)\n",
    "#         unknown_indexes.remove(n)\n",
    "#         if if_plot:\n",
    "#             title = \"Iteration 0\"\n",
    "#             self.plot_svm(clf, X_pool,y_pool, train_indexes, unknown_indexes,sample_preference, False, title)\n",
    "            \n",
    "#         f1_score,mis_cls = self.error_logging(clf,X_test,y_test,)\n",
    "#         score.append(f1_score)\n",
    "#         no_of_err.append(mis_cls)\n",
    "        \n",
    "#         for i in range(budget):\n",
    "#             train_indexes.append(n)\n",
    "#             X_train = X_pool.iloc[train_indexes]\n",
    "#             y_train = y_pool.iloc[train_indexes]\n",
    "            \n",
    "#             if \"QBC\" in self.QueryStrategy:\n",
    "#                 for model in clf:\n",
    "#                     model.fit(X_train, y_train)\n",
    "#             else:\n",
    "#                 clf.fit(X_train, y_train)\n",
    "            \n",
    "#             n = self.Query_Strategy_Frameworks[self.QueryStrategy](clf,X_pool,unknown_indexes)\n",
    "#             unknown_indexes.remove(n)\n",
    "#             if (if_plot)&((i<10)|((i+1)%4==0)):\n",
    "#                 title = \"Iteration \"+str(i+1)\n",
    "#                 self.plot_svm(clf, X_pool,y_pool, train_indexes, unknown_indexes,sample_preference, False, title)\n",
    "#             f1_score,mis_cls = self.error_logging(clf,X_test,y_test,)\n",
    "#             score.append(f1_score)\n",
    "#             no_of_err.append(mis_cls)\n",
    "#         return train_indexes,clf,score,no_of_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Split into Seed and Unlabelled Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.578436Z",
     "start_time": "2021-04-02T13:28:08.573521Z"
    }
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# clf = SVC(kernel='linear',probability=True)\n",
    "\n",
    "# k1, k2, X1, y1,lx0,ly0 = Initialization(clf,'Least_Confident')\n",
    "\n",
    "# X_pool, X_test, y_pool, y_test = train_test_split(X1, y1, test_size=0.2, random_state=2)\n",
    "# X_pool, X_test, y_pool, y_test = X_pool.reset_index(drop=True), X_test.reset_index(drop=True), y_pool.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "# pool_datset = X_pool.copy()\n",
    "# pool_datset['y'] = y_pool\n",
    "\n",
    "# print(f\"X_pool,y_pool: trainning data pool for unlabelled/labelled data ; size = {len(X_pool)}\")\n",
    "# print(f\"X_test,y_test: test data; size = {len(X_test)}\")\n",
    "# display(pool_datset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.584320Z",
     "start_time": "2021-04-02T13:28:08.579572Z"
    }
   },
   "outputs": [],
   "source": [
    "# score_bench_mark = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Train the Model \n",
    "clf = SVC(kernel='linear')\n",
    "# Step 3: Choose unlabelled instances\n",
    "## 1. Determine the type of scenario: Pool-Based sampling\n",
    "## 2. query strategy\n",
    "## 2.0 Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.590191Z",
     "start_time": "2021-04-02T13:28:08.585399Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clf = SVC(kernel='linear',probability=True)\n",
    "# k1, k2, X1, y1,lx0,ly0 = Initialization(clf,'Random_sample')\n",
    "\n",
    "# X_pool, X_test, y_pool, y_test = train_test_split(X1, y1, test_size=0.2, random_state=2)\n",
    "# X_pool, X_test, y_pool, y_test = X_pool.reset_index(drop=True), X_test.reset_index(drop=True), y_pool.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "# pool_datset = X_pool.copy()\n",
    "# pool_datset['y'] = y_pool\n",
    "\n",
    "# print(f\"X_pool,y_pool: trainning data pool for unlabelled/labelled data ; size = {len(X_pool)}\")\n",
    "# print(f\"X_test,y_test: test data; size = {len(X_test)}\")\n",
    "# display(pool_datset.head())\n",
    "\n",
    "\n",
    "# active_learning = active_learning_base('Random_sample')\n",
    "\n",
    "# @active_learning.add_Query_Strategy(active_learning_base)\n",
    "# def Random_sample(clf,X_train,unknown_indexes):\n",
    "#     return random.sample(unknown_indexes, 1)[0]\n",
    "\n",
    "# train_indexes,clf,score,no_of_err = active_learning.active_learner(clf,2,X_pool,y_pool,X_test,y_test,len(y_pool)-3,if_plot=True)\n",
    "# print(score)\n",
    "# print(no_of_err)\n",
    "# score_bench_mark['Random_sample_f1'] =  score\n",
    "# score_bench_mark['Random_sample_err'] =  no_of_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Uncertainty Sampling\n",
    "### 2.1.1 least confident\n",
    "### $x^*_{LC} = argmax_x(1-P_{\\theta}(\\hat{y}|x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.597650Z",
     "start_time": "2021-04-02T13:28:08.595736Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# clf = SVC(kernel='linear',probability=True)\n",
    "\n",
    "# k1, k2, X1, y1,lx0,ly0 = Initialization(clf,'Least_Confident')\n",
    "\n",
    "# X_pool, X_test, y_pool, y_test = train_test_split(X1, y1, test_size=0.2, random_state=2)\n",
    "# X_pool, X_test, y_pool, y_test = X_pool.reset_index(drop=True), X_test.reset_index(drop=True), y_pool.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "# pool_datset = X_pool.copy()\n",
    "# pool_datset['y'] = y_pool\n",
    "\n",
    "# print(f\"X_pool,y_pool: trainning data pool for unlabelled/labelled data ; size = {len(X_pool)}\")\n",
    "# print(f\"X_test,y_test: test data; size = {len(X_test)}\")\n",
    "# display(pool_datset.head())\n",
    "\n",
    "# active_learning = active_learning_base('Least_Confident')\n",
    "\n",
    "# @active_learning.add_Query_Strategy(active_learning_base)\n",
    "# def Least_Confident(clf,X_train,unknown_indexes):\n",
    "#     LC_matrix = [1- max(P_hat) for P_hat in clf.predict_proba(X_train.iloc[unknown_indexes])]\n",
    "#     LC = np.argmax(LC_matrix)\n",
    "#     return unknown_indexes[LC]\n",
    "\n",
    "# train_indexes,clf,score,no_of_err = active_learning.active_learner(clf,2,X_pool,y_pool,X_test,y_test,len(y_pool)-3,if_plot=True)\n",
    "# # print(score)\n",
    "# # print(no_of_err)\n",
    "# score_bench_mark['Least_Confident_f1'] =  score\n",
    "# score_bench_mark['Least_Confident_err'] =  no_of_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 margin sampling\n",
    "### $x^*_{M} = argmax_x(P_{\\theta}(\\hat{y_1}|x)-P_{\\theta}(\\hat{y_2}|x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.614687Z",
     "start_time": "2021-04-02T13:28:08.612851Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# clf = SVC(kernel='linear',probability=True)\n",
    "# k1, k2, X1, y1,lx0,ly0 = Initialization(clf,'Margin_Sampling')\n",
    "\n",
    "# X_pool, X_test, y_pool, y_test = train_test_split(X1, y1, test_size=0.2, random_state=2)\n",
    "# X_pool, X_test, y_pool, y_test = X_pool.reset_index(drop=True), X_test.reset_index(drop=True), y_pool.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "# pool_datset = X_pool.copy()\n",
    "# pool_datset['y'] = y_pool\n",
    "\n",
    "# print(f\"X_pool,y_pool: trainning data pool for unlabelled/labelled data ; size = {len(X_pool)}\")\n",
    "# print(f\"X_test,y_test: test data; size = {len(X_test)}\")\n",
    "# display(pool_datset.head())\n",
    "\n",
    "# active_learning = active_learning_base('Margin_Sampling')\n",
    "\n",
    "# @active_learning.add_Query_Strategy(active_learning_base)\n",
    "# def Margin_Sampling(clf,X_train,unknown_indexes):\n",
    "#     MS_matrix = []\n",
    "#     for P_hat in clf.predict_proba(X_train.iloc[unknown_indexes]):\n",
    "#         P_hat_max = max(P_hat)\n",
    "#         P_hat = np.delete(P_hat, np.argmax(P_hat))\n",
    "#         P_hat_sec = max(P_hat)\n",
    "#         MS_matrix = [P_hat_max - P_hat_sec]\n",
    "#     MS = np.argmin(MS_matrix)\n",
    "#     return unknown_indexes[MS]\n",
    "\n",
    "# train_indexes,clf,score,no_of_err = active_learning.active_learner(clf,2,X_pool,y_pool,X_test,y_test,len(y_pool)-3,if_plot=True)\n",
    "# score_bench_mark['Margin_Sampling_f1'] =  score\n",
    "# score_bench_mark['Margin_Sampling_err'] =  no_of_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3  general uncertainty sampling: entropy\n",
    "### $x^*_{H} = argmax_x(-\\sum_{i}P_{\\theta}(y_i|x)logP_{\\theta}(y_i|x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.622044Z",
     "start_time": "2021-04-02T13:28:08.620142Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# clf = SVC(kernel='linear',probability=True)\n",
    "# k1, k2, X1, y1,lx0,ly0 = Initialization(clf,'sample_by_entropy')\n",
    "\n",
    "# X_pool, X_test, y_pool, y_test = train_test_split(X1, y1, test_size=0.2, random_state=2)\n",
    "# X_pool, X_test, y_pool, y_test = X_pool.reset_index(drop=True), X_test.reset_index(drop=True), y_pool.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "# pool_datset = X_pool.copy()\n",
    "# pool_datset['y'] = y_pool\n",
    "\n",
    "# print(f\"X_pool,y_pool: trainning data pool for unlabelled/labelled data ; size = {len(X_pool)}\")\n",
    "# print(f\"X_test,y_test: test data; size = {len(X_test)}\")\n",
    "# display(pool_datset.head())\n",
    "\n",
    "# active_learning = active_learning_base('sample_by_entropy')\n",
    "\n",
    "# @active_learning.add_Query_Strategy(active_learning_base)\n",
    "# def sample_by_entropy(clf,X_train,unknown_indexes):\n",
    "#     entropy_matrix = []\n",
    "#     y_prob = clf.predict_proba(X_train.iloc[unknown_indexes])\n",
    "#     for y in y_prob:\n",
    "#         entropy = 0.0\n",
    "#         for r in y:\n",
    "#             if r!=0:\n",
    "#                 entropy += r * np.log(r)\n",
    "#         entropy_matrix+=[-entropy]\n",
    "#     return unknown_indexes[np.argmax(entropy_matrix)]\n",
    "\n",
    "# train_indexes,clf,score,no_of_err = active_learning.active_learner(clf,2,X_pool,y_pool,X_test,y_test,len(y_pool)-3,if_plot=True)\n",
    "# score_bench_mark['sample_by_entropy_f1'] =  score\n",
    "# score_bench_mark['sample_by_entropy_err'] =  no_of_err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Query-By-Committee\n",
    "\n",
    "### 2.2.1  Query-By-Committee: vote entropy\n",
    "### $x^*_{VE} = argmax_x(-\\sum_{i}\\frac{V(y_i)}{C}log\\frac{V(y_i)}{C})$\n",
    "* $V(y_i)$: the number of “votes” that a label receives from among the committee members’ predictions\n",
    "* $C$: committee size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.631399Z",
     "start_time": "2021-04-02T13:28:08.629835Z"
    }
   },
   "outputs": [],
   "source": [
    "# from nltk.classify.scikitlearn import SklearnClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# # from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.642802Z",
     "start_time": "2021-04-02T13:28:08.640685Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# committee =[SVC(kernel='linear',probability=True),LogisticRegression(),\n",
    "#             RandomForestClassifier(bootstrap=True),DecisionTreeClassifier()]\n",
    "# k1, k2, X1, y1,lx0,ly0 = Initialization(committee,'QBC_vote_entropy')\n",
    "\n",
    "# X_pool, X_test, y_pool, y_test = train_test_split(X1, y1, test_size=0.2, random_state=2)\n",
    "# X_pool, X_test, y_pool, y_test = X_pool.reset_index(drop=True), X_test.reset_index(drop=True), y_pool.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "# pool_datset = X_pool.copy()\n",
    "# pool_datset['y'] = y_pool\n",
    "\n",
    "# print(f\"X_pool,y_pool: trainning data pool for unlabelled/?labelled data ; size = {len(X_pool)}\")\n",
    "# print(f\"X_test,y_test: test data; size = {len(X_test)}\")\n",
    "# display(pool_datset.head())\n",
    "\n",
    "\n",
    "# active_learning = active_learning_base('QBC_vote_entropy')\n",
    "\n",
    "# @active_learning.add_Query_Strategy(active_learning_base)\n",
    "# def QBC_vote_entropy(committee,X_pool,unknown_indexes):\n",
    "#     X_train = X_pool.iloc[unknown_indexes]\n",
    "#     num_classes = len(committee[0].classes_)\n",
    "#     C = len(committee)\n",
    "#     y_preds = []\n",
    "#     votes = np.zeros((num_classes,len(X_train)))\n",
    "#     for model in committee:\n",
    "#         y_preds = np.fromiter(model.predict(X_train), dtype=np.int)\n",
    "#         for y_idx in range(len(y_preds)):\n",
    "#             votes[y_preds[y_idx]][y_idx]+=1\n",
    "#     votes = votes/C\n",
    "#     entropy_matrix = []\n",
    "#     for vote in votes.T:\n",
    "#         __entropy = 0.0\n",
    "#         for r in vote:\n",
    "#             if r != 0:\n",
    "#                 __entropy += r * np.log(r)\n",
    "#         entropy_matrix+=[-__entropy]\n",
    "#     return unknown_indexes[np.argmax(entropy_matrix)]\n",
    "\n",
    "\n",
    "# train_indexes,clf,score,no_of_err = active_learning.active_learner(committee,2,X_pool,y_pool,X_test,y_test,len(y_pool)-3,if_plot=True)\n",
    "# score_bench_mark['QBC_vote_entropy_f1'] =  score\n",
    "# score_bench_mark['QBC_vote_entropy_f1_err'] =  no_of_err\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1  Query-By-Committee: KL divergence\n",
    "### $x^*_{KL} = argmax_x\\frac{1}{C}\\sum^{C}_{c=1}D(P_{\\theta^{(c)}}||P_C)$\n",
    "* Where:\n",
    "### $D(P_{\\theta^{(c)}}||P_C) = \\sum_{i}P_{\\theta^{(c)}}(y_i|x)log\\frac{P_{\\theta^{(c)}}(y_i|x)}{P_{C}(y_i|x)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.654370Z",
     "start_time": "2021-04-02T13:28:08.652098Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# committee =[SVC(kernel='linear',probability=True),LogisticRegression(),\n",
    "#             RandomForestClassifier(bootstrap=True),DecisionTreeClassifier()]\n",
    "# k1, k2, X1, y1,lx0,ly0 = Initialization(committee,'QBC_kl_divergence')\n",
    "\n",
    "# X_pool, X_test, y_pool, y_test = train_test_split(X1, y1, test_size=0.2, random_state=2)\n",
    "# X_pool, X_test, y_pool, y_test = X_pool.reset_index(drop=True), X_test.reset_index(drop=True), y_pool.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "# pool_datset = X_pool.copy()\n",
    "# pool_datset['y'] = y_pool\n",
    "\n",
    "# print(f\"X_pool,y_pool: trainning data pool for unlabelled/?labelled data ; size = {len(X_pool)}\")\n",
    "# print(f\"X_test,y_test: test data; size = {len(X_test)}\")\n",
    "# display(pool_datset.head())\n",
    "\n",
    "\n",
    "\n",
    "# active_learning = active_learning_base('QBC_kl_divergence')\n",
    "\n",
    "# @active_learning.add_Query_Strategy(active_learning_base)\n",
    "# def QBC_kl_divergence(committee,X_pool,unknown_indexes):\n",
    "#     num_classes = len(committee[0].classes_)\n",
    "#     C = len(committee)\n",
    "#     y_probs = []\n",
    "#     y_probs_C = None\n",
    "#     committee_votes = None\n",
    "    \n",
    "#     for model in committee:\n",
    "#         y_probs += [model.predict_proba(X_pool.iloc[unknown_indexes])]\n",
    "#         if y_probs_C is None:\n",
    "#             y_probs_C = model.predict_proba(X_pool.iloc[unknown_indexes])\n",
    "#         else:\n",
    "#             y_probs_C = y_probs_C + model.predict_proba(X_pool.iloc[unknown_indexes])\n",
    "            \n",
    "#     for y_prob in y_probs:\n",
    "#         votes = []\n",
    "#         for i_idx in range(len(y_prob)):\n",
    "#             vote = 0.0\n",
    "#             PCy_i = y_probs_C[i_idx]\n",
    "#             Py_i = y_prob[i_idx]\n",
    "#             for r_idx in range(len(PCy_i)):\n",
    "#                 if Py_i[r_idx]!=0:\n",
    "#                     vote += Py_i[r_idx] * (np.log(Py_i[r_idx])-np.log(PCy_i[r_idx]))\n",
    "#             votes+=[vote]\n",
    "#         if committee_votes is None:\n",
    "#             committee_votes = np.array(votes)\n",
    "#         else:\n",
    "#             committee_votes = committee_votes + np.array(votes)\n",
    "#     #print(committee_votes)\n",
    "#     committee_votes = np.array(committee_votes)/C\n",
    "#     return unknown_indexes[np.argmax(committee_votes)]\n",
    "\n",
    "# train_indexes,clf,score,no_of_err = active_learning.active_learner(committee,2,X_pool,y_pool,X_test,y_test,len(y_pool)-3,if_plot=True)\n",
    "# score_bench_mark['QBC_kl_divergence_f1'] =  score\n",
    "# score_bench_mark['QBC_kl_divergence_err'] =  no_of_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Expected Model Change\n",
    "\n",
    "### $x^*_{EGL} = argmax_x(P_{\\theta}(y_i|x)\\mathbf{\\triangledown\\ell_\\theta(\\mathcal{L}\\cup\\langle{x,y_i}\\rangle)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.670015Z",
     "start_time": "2021-04-02T13:28:08.668002Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# clf = SVC(kernel='linear',probability=True)\n",
    "# k1, k2, X1, y1,lx0,ly0 = Initialization(clf,'Expected_Model_Change')\n",
    "\n",
    "# X_pool, X_test, y_pool, y_test = train_test_split(X1, y1, test_size=0.2, random_state=2)\n",
    "# X_pool, X_test, y_pool, y_test = X_pool.reset_index(drop=True), X_test.reset_index(drop=True), y_pool.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "# pool_datset = X_pool.copy()\n",
    "# pool_datset['y'] = y_pool\n",
    "\n",
    "# print(f\"X_pool,y_pool: trainning data pool for unlabelled/labelled data ; size = {len(X_pool)}\")\n",
    "# print(f\"X_test,y_test: test data; size = {len(X_test)}\")\n",
    "# display(pool_datset.head())\n",
    "\n",
    "# active_learning = active_learning_base('Expected_Model_Change')\n",
    "\n",
    "# @active_learning.add_Query_Strategy(active_learning_base)\n",
    "# def Expected_Model_Change(clf,X_pool,unknown_indexes):\n",
    "#     changes = []\n",
    "#     P_pred = clf.predict_proba(X_pool.iloc[unknown_indexes])\n",
    "#     y_pred = clf.predict(X_pool.iloc[unknown_indexes])\n",
    "#     X_Labeled_idx = [w for w in X_pool.index if w not in unknown_indexes]\n",
    "#     for i in range(len(unknown_indexes)):\n",
    "#         chg = 0.0\n",
    "#         ukn_idx = unknown_indexes[i]\n",
    "#         m = copy.deepcopy(clf)\n",
    "#         X_idx = X_Labeled_idx.copy()\n",
    "#         X_idx.append(ukn_idx)\n",
    "#         m = m.fit(X_pool.iloc[X_idx], np.append(y_pool.iloc[X_Labeled_idx],y_pred[i]))\n",
    "#         new_y_pred = m.predict(X_pool.iloc[unknown_indexes])\n",
    "#         new_P_pred = m.predict_proba(X_pool.iloc[unknown_indexes])\n",
    "        \n",
    "#         label_change = np.abs(np.power(new_y_pred, 2) - np.power(y_pred, 2))\n",
    "#         prediction_change = np.abs(np.apply_along_axis(np.max, 1, new_P_pred) - np.apply_along_axis(np.max, 1,P_pred))\n",
    "        \n",
    "#         chg = np.sum(np.array([a*b for a,b in zip(label_change,prediction_change)]))\n",
    "#         changes.append(chg)\n",
    "#     return unknown_indexes[np.argmax(changes)]\n",
    "\n",
    "# train_indexes,clf,score,no_of_err = active_learning.active_learner(clf,2,X_pool,y_pool,X_test,y_test,len(y_pool)-3,if_plot=True)\n",
    "# score_bench_mark['Expected_Model_Change_f1'] =  score\n",
    "# score_bench_mark['Expected_Model_Change_err'] =  no_of_err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Expected Error Reduction\n",
    "\n",
    "## 2.4.1 0/1 loss\n",
    "### $x^*_{0/1} = argmax_x(P_{\\theta}(y_i|x)\t( \\,\\sum_{u=1}^{U}1-P_{\\theta^{+}\\langle{x,y_i}\\rangle}(\\hat{y}|x^{(u)}))\\,$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.685719Z",
     "start_time": "2021-04-02T13:28:08.683031Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# clf = SVC(kernel='linear',probability=True)\n",
    "# k1, k2, X1, y1,lx0,ly0 = Initialization(clf,'Expected_Error_Reduction_01_loss')\n",
    "\n",
    "# X_pool, X_test, y_pool, y_test = train_test_split(X1, y1, test_size=0.2, random_state=2)\n",
    "# X_pool, X_test, y_pool, y_test = X_pool.reset_index(drop=True), X_test.reset_index(drop=True), y_pool.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "# pool_datset = X_pool.copy()\n",
    "# pool_datset['y'] = y_pool\n",
    "\n",
    "# print(f\"X_pool,y_pool: trainning data pool for unlabelled/labelled data ; size = {len(X_pool)}\")\n",
    "# print(f\"X_test,y_test: test data; size = {len(X_test)}\")\n",
    "# display(pool_datset.head())\n",
    "\n",
    "# active_learning = active_learning_base('Expected_Error_Reduction_01_loss')\n",
    "\n",
    "# @active_learning.add_Query_Strategy(active_learning_base)\n",
    "# def Expected_Error_Reduction_01_loss(clf,X_pool,unknown_indexes,loss=\"0_1_loss\"):\n",
    "#     Errors = []\n",
    "#     classes = np.unique(y_pool)\n",
    "#     n_classes = len(classes)\n",
    "    \n",
    "#     P_preds = clf.predict_proba(X_pool.iloc[unknown_indexes])\n",
    "    \n",
    "#     X_Labeled_idx = [w for w in X_pool.index if w not in unknown_indexes]\n",
    "#     for i in range(len(unknown_indexes)):\n",
    "#         err = []\n",
    "#         ukn_idx = unknown_indexes[i]\n",
    "#         X_idx = X_Labeled_idx.copy()\n",
    "#         X_idx.append(ukn_idx)\n",
    "        \n",
    "#         for yi in range(n_classes):\n",
    "#             m = copy.deepcopy(clf)\n",
    "#             m = m.fit(X_pool.iloc[X_idx], np.append(y_pool.iloc[X_Labeled_idx],yi))\n",
    "#             new_P_preds = m.predict_proba(X_pool.iloc[unknown_indexes])\n",
    "#             if loss == '0_1_loss':  # 0/1 loss\n",
    "#                 err.append(P_preds[i, yi] * np.sum(1-np.max(new_P_preds, axis=1)))\n",
    "#             elif loss == 'log': # log loss\n",
    "#                 err.append(P_preds[i, yi] * - np.sum(new_P_preds * np.log(new_P_preds)))\n",
    "#         Errors.append(np.sum(err))\n",
    "# #         print(len(Errors))\n",
    "# #         print(len(unknown_indexes))\n",
    "#     return unknown_indexes[np.argmin(Errors)]\n",
    "\n",
    "# train_indexes,clf,score,no_of_err = active_learning.active_learner(clf,2,X_pool,y_pool,X_test,y_test,len(y_pool)-3,if_plot=True)\n",
    "# score_bench_mark['Expected_Error_Reduction_01_loss_f1'] =  score\n",
    "# score_bench_mark['Expected_Error_Reduction_01_loss_err'] =  no_of_err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.2 log loss\n",
    "### $x^*_{log} = argmax_x(P_{\\theta}(y_i|x)\t(\\,-\\sum_{u=1}^{U}\\sum_{j}P_{\\theta^{+}\\langle{x,y_i}\\rangle}(\\hat{y_j}|x^{(u)})logP_{\\theta^{+}\\langle{x,y_i}\\rangle}(\\hat{y_j}|x^{(u)}))\\,$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.700220Z",
     "start_time": "2021-04-02T13:28:08.697826Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# clf = SVC(kernel='linear',probability=True)\n",
    "# k1, k2, X1, y1,lx0,ly0 = Initialization(clf,'Expected_Error_Reduction_log_loss')\n",
    "\n",
    "# X_pool, X_test, y_pool, y_test = train_test_split(X1, y1, test_size=0.2, random_state=2)\n",
    "# X_pool, X_test, y_pool, y_test = X_pool.reset_index(drop=True), X_test.reset_index(drop=True), y_pool.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "# pool_datset = X_pool.copy()\n",
    "# pool_datset['y'] = y_pool\n",
    "\n",
    "# print(f\"X_pool,y_pool: trainning data pool for unlabelled/labelled data ; size = {len(X_pool)}\")\n",
    "# print(f\"X_test,y_test: test data; size = {len(X_test)}\")\n",
    "# display(pool_datset.head())\n",
    "\n",
    "# active_learning = active_learning_base('Expected_Error_Reduction_log_loss')\n",
    "\n",
    "# @active_learning.add_Query_Strategy(active_learning_base)\n",
    "# def Expected_Error_Reduction_log_loss(clf,X_pool,unknown_indexes,loss=\"log\"):\n",
    "#     Errors = []\n",
    "#     classes = np.unique(y_pool)\n",
    "#     n_classes = len(classes)\n",
    "    \n",
    "#     P_preds = clf.predict_proba(X_pool.iloc[unknown_indexes])\n",
    "    \n",
    "#     X_Labeled_idx = [w for w in X_pool.index if w not in unknown_indexes]\n",
    "#     for i in range(len(unknown_indexes)):\n",
    "#         err = []\n",
    "#         ukn_idx = unknown_indexes[i]\n",
    "#         X_idx = X_Labeled_idx.copy()\n",
    "#         X_idx.append(ukn_idx)\n",
    "        \n",
    "#         for yi in range(n_classes):\n",
    "#             m = copy.deepcopy(clf)\n",
    "#             m = m.fit(X_pool.iloc[X_idx], np.append(y_pool.iloc[X_Labeled_idx],yi))\n",
    "#             new_P_preds = m.predict_proba(X_pool.iloc[unknown_indexes])\n",
    "#             if loss == '0_1_loss':  # 0/1 loss\n",
    "#                 err.append(P_preds[i, yi] * np.sum(1-np.max(new_P_preds, axis=1)))\n",
    "#             elif loss == 'log': # log loss\n",
    "#                 err.append(P_preds[i, yi] * - np.sum(new_P_preds * np.log(new_P_preds)))\n",
    "#         Errors.append(np.sum(err))\n",
    "# #         print(len(Errors))\n",
    "# #         print(len(unknown_indexes))\n",
    "#     return unknown_indexes[np.argmin(Errors)]\n",
    "\n",
    "# train_indexes,clf,score,no_of_err = active_learning.active_learner(clf,2,X_pool,y_pool,X_test,y_test,len(y_pool)-3,if_plot=True)\n",
    "# score_bench_mark['Expected_Error_Reduction_log_loss_f1'] =  score\n",
    "# score_bench_mark['Expected_Error_Reduction_log_loss_err'] =  no_of_err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.708765Z",
     "start_time": "2021-04-02T13:28:08.706767Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# clf = SVC(kernel='linear',probability=True)\n",
    "# k1, k2, X1, y1,lx0,ly0 = Initialization(clf,'Random_sample')\n",
    "\n",
    "# X_pool, X_test, y_pool, y_test = train_test_split(X1, y1, test_size=0.2, random_state=2)\n",
    "# X_pool, X_test, y_pool, y_test = X_pool.reset_index(drop=True), X_test.reset_index(drop=True), y_pool.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "# pool_datset = X_pool.copy()\n",
    "# pool_datset['y'] = y_pool\n",
    "\n",
    "# print(f\"X_pool,y_pool: trainning data pool for unlabelled/labelled data ; size = {len(X_pool)}\")\n",
    "# print(f\"X_test,y_test: test data; size = {len(X_test)}\")\n",
    "# display(pool_datset.head())\n",
    "\n",
    "\n",
    "# active_learning = active_learning_base('Random_sample')\n",
    "\n",
    "# @active_learning.add_Query_Strategy(active_learning_base)\n",
    "# def Random_sample(clf,X_train,unknown_indexes):\n",
    "#     random.seed(1544514)#54315\n",
    "#     return random.sample(unknown_indexes, 1)[0]\n",
    "\n",
    "# train_indexes,clf,score,no_of_err = active_learning.active_learner(clf,2,X_pool,y_pool,X_test,y_test,len(y_pool)-3,if_plot=True)\n",
    "# print(score)\n",
    "# print(random.getstate())\n",
    "# # score_bench_mark['Random_sample'] =  score\n",
    "# score_bench_mark['Random_sample_f1'] =  score\n",
    "# score_bench_mark['Random_sample_err'] =  no_of_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.714392Z",
     "start_time": "2021-04-02T13:28:08.711832Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clf = SVC(kernel='linear',probability=True)\n",
    "# k1, k2, X1, y1,lx0,ly0 = Initialization(clf,'Random_sample')\n",
    "\n",
    "# X_pool, X_test, y_pool, y_test = train_test_split(X1, y1, test_size=0.2, random_state=2)\n",
    "# X_pool, X_test, y_pool, y_test = X_pool.reset_index(drop=True), X_test.reset_index(drop=True), y_pool.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "# pool_datset = X_pool.copy()\n",
    "# pool_datset['y'] = y_pool\n",
    "\n",
    "# print(f\"X_pool,y_pool: trainning data pool for unlabelled/labelled data ; size = {len(X_pool)}\")\n",
    "# print(f\"X_test,y_test: test data; size = {len(X_test)}\")\n",
    "# display(pool_datset.head())\n",
    "\n",
    "# active_learning = active_learning_base('Random_sample')\n",
    "\n",
    "# @active_learning.add_Query_Strategy(active_learning_base)\n",
    "# def Random_sample(clf,X_pool,unknown_indexes):\n",
    "#     changes = []\n",
    "#     P_pred = [[max(i)] for i in clf.predict_proba(X_pool.iloc[unknown_indexes])]\n",
    "#     y_pred = clf.predict(X_pool.iloc[unknown_indexes])\n",
    "#     X_Labeled_idx = [w for w in X_pool.index if w not in unknown_indexes]\n",
    "#     for i in range(len(unknown_indexes)):\n",
    "#         chg = 0.0\n",
    "#         ukn_idx = unknown_indexes[i]\n",
    "#         m = copy.deepcopy(clf)\n",
    "#         X_idx = X_Labeled_idx.copy()\n",
    "#         X_idx.append(ukn_idx)\n",
    "#         m = m.fit(X_pool.iloc[X_idx], np.append(y_pool.iloc[X_Labeled_idx],y_pred[i]))\n",
    "#         new_y_pred = m.predict(X_pool.iloc[unknown_indexes])\n",
    "#         label_change =np.multiply(np.abs(np.power(new_y_pred, 2) - np.power(y_pred, 2)), P_pred)\n",
    "#         chg = np.sum(label_change)          \n",
    "#         changes.append(chg)\n",
    "#     return unknown_indexes[np.argmin(changes)]\n",
    "\n",
    "# train_indexes,clf,score,no_of_err = active_learning.active_learner(clf,2,X_pool,y_pool,X_test,y_test,len(y_pool)-3,if_plot=True)\n",
    "\n",
    "# score_bench_mark['Random_sample_f1'] =  score\n",
    "# score_bench_mark['Random_sample_err'] =  no_of_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.719523Z",
     "start_time": "2021-04-02T13:28:08.715672Z"
    }
   },
   "outputs": [],
   "source": [
    "# score_bench_mark.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.725047Z",
     "start_time": "2021-04-02T13:28:08.721162Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame.from_dict(score_bench_mark).reset_index()\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.730228Z",
     "start_time": "2021-04-02T13:28:08.726443Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame.from_dict(score_bench_mark).reset_index()\n",
    "# df = df.round(2)\n",
    "# from scipy.ndimage.filters import gaussian_filter1d\n",
    "# for col in df.columns[1:]:\n",
    "#     #df[col+'_MA'] = df.loc[:,col].rolling(window=5).mean()\n",
    "#     df[col+'_smooth'] = gaussian_filter1d(df[col], sigma=2)\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.735439Z",
     "start_time": "2021-04-02T13:28:08.731558Z"
    }
   },
   "outputs": [],
   "source": [
    "# import plotly.express as px \n",
    "# df1 = pd.melt(df, id_vars=['index'], value_vars=[col for col in list(df.columns) if col[-4:]==\"_err\"]).reset_index().rename(columns={\"index\": \"Iteration\", \"value\": \"err\",\"variable\":\"Query_Strategy\"})\n",
    "# fig = px.line(df1, x='Iteration', y='err', color='Query_Strategy')\n",
    "# # df2 = pd.melt(df, id_vars=['index'], value_vars=list(df.columns)[7:])\n",
    "# # fig = px.line(df2, x='index', y='value', color='variable')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.740599Z",
     "start_time": "2021-04-02T13:28:08.736750Z"
    }
   },
   "outputs": [],
   "source": [
    "# import plotly.express as px \n",
    "# df1 = pd.melt(df, id_vars=['index'], value_vars=[col for col in list(df.columns) if col[-11:]==\"_err_smooth\"]).reset_index().rename(columns={\"index\": \"Iteration\", \"value\": \"err_smooth\",\"variable\":\"Query_Strategy\"})\n",
    "# fig = px.line(df1, x='Iteration', y='err_smooth', color='Query_Strategy')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.746492Z",
     "start_time": "2021-04-02T13:28:08.741705Z"
    }
   },
   "outputs": [],
   "source": [
    "# subfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# df1 = pd.melt(df, id_vars=['index'], value_vars=[col for col in list(df.columns) if col[-4:]==\"_err\"]).reset_index().rename(columns={\"index\": \"Iteration\", \"value\": \"err\",\"variable\":\"Query_Strategy\"})\n",
    "\n",
    "# fig1 = px.scatter(df1, x='Iteration', y='err', color='Query_Strategy')\n",
    "\n",
    "# fig1.update_traces(marker=dict(size=5),\n",
    "#                   selector=dict(mode='markers'))\n",
    "\n",
    "\n",
    "# df2 = pd.melt(df, id_vars=['index'], value_vars=[col for col in list(df.columns) if col[-11:]==\"_err_smooth\"]).reset_index().rename(columns={\"index\": \"Iteration\", \"value\": \"err_smooth\",\"variable\":\"Query_Strategy\"})\n",
    "\n",
    "# fig2 = px.line(df2, x='Iteration', y='err_smooth', color='Query_Strategy')\n",
    "# fig2.update_traces(yaxis=\"y2\")\n",
    "\n",
    "# subfig.add_traces(fig1.data + fig2.data)\n",
    "# subfig.layout.xaxis.title=\"Iteration\"\n",
    "# subfig.layout.yaxis.title=\"Error\"\n",
    "# subfig.layout.yaxis2.title=\"Error_smooth\"\n",
    "\n",
    "# subfig.update_layout(legend=dict(\n",
    "#     orientation=\"h\",\n",
    "#     yanchor=\"bottom\",\n",
    "#     y=1.02,\n",
    "#     xanchor=\"right\",\n",
    "#     x=1\n",
    "# ))\n",
    "\n",
    "# subfig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:28:08.751798Z",
     "start_time": "2021-04-02T13:28:08.747929Z"
    }
   },
   "outputs": [],
   "source": [
    "# subfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# df1 = pd.melt(df, id_vars=['index'], value_vars=[col for col in list(df.columns) if col[-3:]==\"_f1\"]).reset_index().rename(columns={\"index\": \"Iteration\", \"value\": \"f1\",\"variable\":\"Query_Strategy\"})\n",
    "\n",
    "# fig1 = px.scatter(df1, x='Iteration', y='f1', color='Query_Strategy')\n",
    "\n",
    "# fig1.update_traces(marker=dict(size=5),\n",
    "#                   selector=dict(mode='markers'))\n",
    "\n",
    "\n",
    "# df2 = pd.melt(df, id_vars=['index'], value_vars=[col for col in list(df.columns) if col[-10:]==\"_f1_smooth\"]).reset_index().rename(columns={\"index\": \"Iteration\", \"value\": \"f1_smooth\",\"variable\":\"Query_Strategy\"})\n",
    "\n",
    "# fig2 = px.line(df2, x='Iteration', y='f1_smooth', color='Query_Strategy')\n",
    "# fig2.update_traces(yaxis=\"y2\")\n",
    "\n",
    "# subfig.add_traces(fig1.data + fig2.data)\n",
    "# subfig.layout.xaxis.title=\"Iteration\"\n",
    "# subfig.layout.yaxis.title=\"F1\"\n",
    "# subfig.layout.yaxis2.title=\"F1_smooth\"\n",
    "\n",
    "# subfig.update_layout(legend=dict(\n",
    "#     orientation=\"h\",\n",
    "#     yanchor=\"bottom\",\n",
    "#     y=1.02,\n",
    "#     xanchor=\"right\",\n",
    "#     x=1\n",
    "# ))\n",
    "\n",
    "# subfig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f10ds_spark_py36",
   "language": "python",
   "name": "f10ds_spark_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
